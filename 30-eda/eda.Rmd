---
title: "Análisis exploratorio de datos"
output: 
  html_document:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: false
    # theme: flatly
# editor_options: 
#   chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T)
```

Vamos a explorar datos de los municipios de Brasil de 2019 de tres fuentes:

* Pagos del plan Bolsa Familia del Portal de Transparencia (https://dados.gov.br/dataset/bolsa-familia-pagamentos) 
* El [Instituto Brasileño de Geografía y Estadística](https://dados.gov.br/organization/about/instituto-brasileiro-de-geografia-e-estatistica-ibge) (IBGE)
* El [Instituto de Investigación Económica Aplicada](https://dados.gov.br/organization/about/ipea)

```{r, message=F, warning=F}
library(tidyverse)
library(janitor)
library(skimr)
# library(kableExtra) # dont load it because it breaks skimr
library(GGally)
library(ggpubr)

# remotes::install_github("rpkgs/gg.layers")
library(gg.layers)
```

Levantamos los tres datasets:

```{r}
file_bolsa = "data/working/bolsafamilia_municipios_2019.csv"
file_ibge = "data/working/ibge_municipios_2019.csv"
file_varios = "data/working/varios_municipios_2019.csv"
```


```{r}
bolsa = read_csv(file_bolsa, show_col_types=F)
ibge = read_csv(file_ibge, show_col_types=F)
varios = read_csv(file_varios, show_col_types=F)
```


# Inspección inicial

Inspeccionamos algunas características básicas de los data.frames:

```{r}
glimpse(bolsa)
head(bolsa)
```

```{r}
glimpse(ibge)
```

```{r}
glimpse(varios)
```

Algunas notas sobre los datos:

* `UF` (Unidad Federativa) indica el estado.
* `num_beneficiarios_mean` indica el número de beneficiarios promedio a lo largo del año.
* `suma_valor` indica la suma de los pagos totales en el año.
* `despesa_pessoal_encargos_sociais` indica gasto en personal y cargas sociales.
* el nombre del municipio `municipio` ya está normalizado entre los datasets.

# Operaciones básicas 

Podemos usar funciones para conservar filas según algún criterio.

```{r}
bolsa %>% 
  dplyr::filter(uf == "RJ" & num_beneficiarios_mean > 50000)
bolsa %>% 
  arrange(desc(uf), -suma_valor) %>% 
  head()
bolsa %>% slice(20:25)
```

También podemos hacer operaciones sobre las columnas:

```{r}
ibge %>% select(municipio, vab_agro) %>% head()
ibge %>% 
  select(-c(nome_da_grande_regiao,nome_da_unidade_da_federacao)) %>% 
  head()
ibge %>% select(municipio) %>% sample_n(3)
ibge["municipio"] %>% sample_n(3)

ibge$municipio %>% sample(3)
ibge %>% pull(municipio) %>% sample(3)
ibge[["municipio"]] %>% sample(3)
```

Podemos renombrar columnas:

```{r}
ibge = ibge %>%
  rename(
    nome_regiao = nome_da_grande_regiao
    ,nome_uf = nome_da_unidade_da_federacao
    ,hierarquia = hierarquia_urbana_principais_categorias
    ,tipologia = tipo_urbano
  )
```

Para normalizar o limpiar los nombres de las columnas es muy útil la función `janitor::clean_names()` (en este caso ya la aplicamos cuando generamos los datasets).

Esta librería también tiene otras funciones muy útiles, como `remove_empty()` para eliminar filas o columnas vacías o `remove_constant()` para eliminar filas o columnas constantes.

Podemos hacer un join de los datasets usando `municipio` como clave. Antes de hacerlo es conveniente revisar si hay observaciones duplicadas o si algún dataset tiene observaciones faltantes.

## Chequeo de duplicados

```{r}
dups_bolsa = bolsa %>% get_dupes("municipio")
dups_ibge = ibge %>% get_dupes("municipio")
dups_varios = varios %>% get_dupes("municipio")
```

```{r}
munis_bolsa = bolsa %>% pull(municipio) %>% unique()
munis_ibge = ibge %>% pull(municipio) %>% unique()
munis_varios = varios %>% pull(municipio) %>% unique()

cat(
  length(munis_bolsa),
  length(munis_ibge),
  length(munis_varios)
)
```


```{r}
length(intersect(munis_bolsa, munis_ibge))
```

```{r}
setdiff(munis_varios, munis_ibge)
```
```{r}
setdiff(munis_ibge, munis_varios)
```


```{r}
# aplica la funcion intersect recursivamente sobre la lista
munis_en_comun = Reduce(
  intersect, list(munis_bolsa, munis_ibge, munis_varios))
length(munis_en_comun)
# Reduce(fn, elementos)
```

---------------------------------------------------------------

Hacemos el join.

```{r}
df = ibge %>%
  full_join(bolsa, by="municipio") %>% # 5570 municipios 
  left_join(varios, by="municipio") 
  # excluimos los que estan en varios que no estan en los otros dos (27 municipios)
```

Podemos crear nuevas columnas relevantes con `mutate()`.

```{r}
df = df %>%
  mutate(
    densidad = populacao_2019 / area_geografica_2019
    ,suma_perbenef = suma_valor / num_beneficiarios_mean
  ) 
```


```{r}
df = df %>% 
  mutate_at(
    vars(
      vab_agro, vab_industria, despesa_ciencia_tecnologia_2019
      ,despesa_pessoal_encargos_sociais_2019, despesa_educacao_cultura_2019
      ,despesa_transporte_2019, exportacoes_2019, importacoes_2019
      ,num_beneficiarios_mean
    ), list(pc=function(x) x / .$populacao_2019)
  )
```


```{r}
df = df %>% 
  mutate_at(vars(codigo_municipio, codigo_uf), as.character)
```

Podemos crear categorías nuevas en función de valores de otras variables con `ifelse()` y `case_when()`

```{r}
df = df %>% 
  mutate(
    semiarido = ifelse(semiarido == "Sim", T, F)
  )
```


```{r}
df = df %>% 
  mutate(
    regiao = case_when(
      nome_regiao == "Norte" ~ "N"
      ,nome_regiao == "Nordeste" ~ "NE"
      ,nome_regiao == "Centro-oeste" ~ "CO"
      ,nome_regiao == "Sudeste" ~ "SE"
      ,nome_regiao == "Sul" ~ "S"
      ,TRUE ~ "error"
    )
  )
```

Podemos usar `forcats::fct_lump()` para agrupar categorías poco frecuentes.

```{r}
df$atividade_maior_vab %>% unique()
df %>% 
  mutate(
    atividade = forcats::fct_lump(
      atividade_maior_vab, prop=0.1, other_level="Otros")
  ) %>% 
  pull(atividade) %>% 
  unique()
```

Calculamos algunas medidas de resumen, globales y por grupo:

```{r}
df %>% summarise(
  suma_media = mean(suma_valor)
  ,pop_max = max(populacao_2019, na.rm=T)
  ,area_min = min(area_geografica_2019, na.rm=T)
)
```

```{r}
df %>% 
  summarise_if(is.numeric, list(media=mean, max=max, min=min))
# summarise_at --> indicar por nombre
```

Las data.frames pueden estar en formato "long" o "wide": 

```{r}
tab = df %>% 
  group_by(nome_uf, semiarido) %>% 
  summarise(
    pop = sum(populacao_2019, na.rm=T)
    ,n = n()
  ) %>% 
  ungroup() %>% 
  arrange(-pop)

tab
```

```{r}
# long to wide
tab_wide = tab %>% 
  pivot_wider(id_cols=nome_uf, names_from=semiarido, values_from=c(pop,n))
tab_wide %>% head()
```


```{r}
# wide to long
tab_long = tab %>% 
  pivot_longer(cols=-c(nome_uf, semiarido), names_to="variable", values_to="value")
tab_long %>% head()
```

Para buscar nombres de variables podemos usar:

```{r}
df %>% names() %>% str_subset("despesa")
```

Vamos a eliminar el sufijo del año en los nombres de las columnas porque sabemos que todos los datos son del mismo año:

```{r}
df = df %>% rename_all(function(x) str_remove(x, "_2019"))
```

# Exploratorio

a.k.a. _EDA_ (Exploratory Data Analysis)

Vamos a suponer que nos interesa inferir o predecir el PIB a nivel municipio. Esto podría ser útil, por ejemplo, si sabemos que para algunos municipios este dato es poco confiable, mientras para otros no. También si es un dato que se publica con rezago con respecto a otros que están disponibles más rápido.

**WARNING**: esto es solamente un escenario hipótetico. Si de verdad quisiéramos hacer este ejercicio deberíamos analizar muchos aspectos, como:

* ¿Cómo se estima el PIB? ¿Hay algún método más simple que me permita estimar el dato sin machine learning?
* ¿La población de municipios sin dato confiable tiene una distribución similar en sus features a la población con dato confiable? En tal caso, ¿podemos suponer que el DGP es el mismo?
* ¿Las covariables van a estar disponibles al momento de la predicción?
* ¿Hay otros features importantes disponibles?
* Etc.

Vamos a usar algunas funciones para tener un vistazo general de los datos.

```{r}
skim(df)
```

```{r}
skim_eda = partition(skim(df))
names(skim_eda)
```

Revisar [la vignette de `skimr`](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html)!

A veces es útil separar el dataset según el tipo de variables.

```{r}
df_num = df %>% select_if(is.numeric)
df_cat = df %>% select_if(function(x) !is.numeric(x))
```

## Análisis de datos faltantes

Calculamos el % de datos faltantes por variable y por municipio:

```{r}
map_dbl(df, function(x) mean(is.na(x)) * 100) %>% 
  sort(decreasing=T) %>% 
  as.data.frame()
```

```{r}
na_by_row = df %>% apply(1, function(x) mean(is.na(x)) * 100)
names(na_by_row) = df$municipio
na_by_row %>% sort(decreasing=T) %>% head(10)
mean(na_by_row > 0)
```

También podemos recurrir a `skimr`:

```{r}
skim_eda$numeric %>% 
  select(skim_variable, n_missing) %>% 
  filter(n_missing > 0)
```

```{r}
skim_eda$character %>% 
  select(skim_variable, n_missing) %>% 
  filter(n_missing > 0)
```

En un caso real hay que ser muy cuidadoso con los datos faltantes. Además, el tratamiento que les demos depende del tipo de modelo predictivo que usemos: en algunos casos vamos a necesitar imputarlos mientras que ciertas metodologías admiten NAs.

En la exploración de datos es fundamental entender la interpretación de los _missing_ variable por variable. 

En este ejemplo vamos a suponer que los NA en las variables monetarias indican que el monto es igual a 0. 

```{r}
vars_monetarias = c(
  "despesa_ciencia_tecnologia", "despesa_educacao_cultura", 
  "despesa_pessoal_encargos_sociais", "despesa_transporte",
  "exportacoes", "importacoes", "pib", "pib_percapita", 
  "suma_valor", "vab_agro", "vab_industria"
)
df = df %>% 
  mutate_at(vars_monetarias, function(x) ifelse(is.na(x), 0, x))
```

Además vemos que los datos de población con NA se pueden reconstruir con el PIB y el PIB per cápita.

```{r}
df = df %>% 
  mutate(
    populacao = ifelse(
      is.na(populacao), pib / pib_percapita * 1000, populacao)
    ,populacao = round(populacao, 0)
  )
```

Ahora deberíamos recalcular las variables per cápita (moraleja: ¡primero analizar NA y luego calcular variables!):

```{r}
vars_to_recompute = vars_monetarias[
  !vars_monetarias %in% c("pib", "pib_percapita")
]
df = df %>% 
  mutate_at(
    vars_to_recompute, list(pc=function(x) x /.$populacao)
  )

df$densidad = df$populacao / df$area_geografica
```

Y volver a separar en cat. y num.

```{r}
df_num = df %>% select_if(is.numeric)
df_cat = df %>% select_if(function(x) !is.numeric(x))
```

## Variables categóricas

Revisamos la distribución de frecuencias de las variables categóricas.

```{r}
table(df_cat$tipologia)
table(df_cat$tipologia, df_cat$semiarido)
```

```{r}
# para todas las variables
tabs = df_cat %>% map(function(x) table(x, useNA="ifany"))
tabs$hierarquia
```

```{r}
tabs$tipologia
```


Podemos hacer tablas más informativas con `janitor::tabyl()`.

```{r}
df_cat %>% 
  tabyl(tipologia, semiarido) %>% 
  adorn_totals() %>% 
  adorn_percentages("row") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns()
```

Cortamos el target numérico en bins y hacemos tablas de frecuencias:

```{r}
tablas_vs_target = function(target, df_cat, variable, g=5) {
  df_cat$target_ = Hmisc::cut2(target, g=g)
  tab = df_cat %>% 
    tabyl(target_, !!sym(variable)) %>% 
    adorn_totals() %>% 
    adorn_percentages("col") %>% 
    adorn_pct_formatting() %>% 
    adorn_ns()
  return(tab)
}

tablas = list()
for (col in names(df_cat)) {
  tablas[[col]] = tablas_vs_target(
    df$pib_percapita, df_cat, col)
}
```

```{r}
tablas$hierarquia
```


Las podemos hacer más lindas con `knitr::kable()` y `kableExtra::kable_styling()`.

```{r}
tablas$hierarquia %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(
    bootstrap_options=c("striped", "hover"), font_size=12
  )
```

¡Y aún más lindas con [`gt`](https://gt.rstudio.com/articles/intro-creating-gt-tables.html) o [`gtsummary`](https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)!

## Variables numéricas

Cálculo de cuantiles de variables numéricas:

```{r}
quantile(df_num$pib_percapita, seq(0,1,0.1))
```

```{r}
list_cuantiles = df_num %>% 
  map(function(x) quantile(x, seq(0, 1, 0.25), na.rm=T) )
```

```{r}
list_cuantiles %>% 
  as.data.frame() %>% 
  select(num_beneficiarios_mean, suma_valor, pib)
```

¡También podemos revisar los resultados de `skimr`!

### Correlaciones

Con un correlograma visualizamos la correlación entre todas las variables cuantitativas:

```{r}
GGally::ggcorr(
  df_num, method=c("pairwise","spearman"),  
  label=T, hjust=1, label_size=2, layout.exp=10, size=3)
```

Las correlaciones de a pares pueden ser útiles para detectar variables redundantes:

```{r}
cor_matrix = cor(df_num, method="spearman", use="pairwise")
cor_matrix[upper.tri(cor_matrix, diag=T)] = NA
df_cor = cor_matrix %>% as.table() %>% as.data.frame()
df_cor %>% 
  rename(corr = Freq) %>% 
  filter(!is.na(corr) & Var1 != Var2) %>% 
  arrange(-abs(corr)) %>% 
  head(10) %>% 
  knitr::kable() %>%
  kableExtra::kable_styling()

```


Métodos de manejo de NA en `cor()`:

* `"everything"`: usa _todas_ las observaciones (si hay NA en un par de variables, arroja NA)
* `"pairwise"`: usa las observaciones completas de a pares (descarta las observaciones con algún NA de a pares)
* `"complete.obs"`: usa las observaciones completas del data.frame (descarta las observaciones con algún NA considerando todas las filas)


## Visualizaciones

### Histogramas y densidades

```{r}
ggplot(df, aes(x=pib_percapita)) +
  geom_histogram(alpha=0.5) +
  NULL
```

```{r}
ggplot(df, aes(x=log(pib_percapita), fill=semiarido)) +
  geom_histogram(alpha=0.5) +
  NULL
```

```{r}
ggplot(df, aes(x=log(pib_percapita), fill=semiarido)) +
  geom_density(alpha=0.5, adjust=2) +
  NULL
```

```{r}
ggplot(df, aes(x=log(pib_percapita), y=tipologia, fill =..x..)) +
  ggridges::geom_density_ridges_gradient(scale=1) +
  scale_fill_viridis_c() +
  NULL
```

```{r}
ggplot(df, aes(x=log(pib_percapita), y=tipologia, fill=tipologia)) +
  ggridges::stat_density_ridges(
    quantile_lines=T, quantiles=c(.25,.5,.75), alpha=0.7) +
  NULL
```

```{r}
ggplot(df, aes(x=log(pib_percapita), y=tipologia, fill=tipologia)) +
  ggridges::geom_density_ridges(jittered_points=T, point_size=0.5) +
  NULL
```


### Boxplots

![Fuente: https://r4ds.had.co.nz/](img/boxplot_r4ds.png)

```{r}
ggplot(df, aes(x=nome_regiao, y=pib_percapita, fill=nome_regiao)) +
  geom_boxplot() + 
  scale_y_continuous(trans='log10') +
  facet_wrap(~ tipologia) +
  theme_minimal() +
  NULL
```

```{r}
ggplot(df, aes(x=nome_regiao, y=pib_percapita, fill=nome_regiao)) +
  gg.layers::geom_boxplot2() + 
  scale_y_continuous(trans='log10') +
  facet_wrap(~ tipologia) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position="none") +
  NULL
```

### Scatter plots

```{r}
ggplot(df, aes(x=suma_perbenef, y=pib_percapita, color=nome_regiao)) +
  geom_point() +
  NULL
```

```{r}
ggplot(df, aes(x=log(suma_perbenef), y=log(pib_percapita))) +
  geom_point(alpha=0.3, size=0.5) +
  geom_smooth() + # smoother / suavizado
  NULL
```

```{r}
ggplot(df, 
       aes(x=log10(suma_perbenef), y=log10(pib_percapita), color=nome_regiao)) +
  geom_point() +
  NULL
```

La escala logarítmica es una escala no lineal en la que moverse una unidad implica multiplicarse por las base del logaritmo (10 o _e_, por ejemplo). $\text{log}_{10}$ mide, por ejemplo, órdenes de magnitud ($x$ difiere en un orden de magnitud de $y$ si $x$ es aproximadamente 10 veces mayor que $y$). 

Entonces, la transformación logarítmica es útil cuando la variabilidad relevante está en el orden de magnitud, no en la escala original. Esto es común en el caso de variables monetarias, cuando las variaciones relevantes son porcentuales y no nominales. 

Una relación lineal en un gráfico log-log implica elasticidad constante: la pendiente indica aproximadamente cuánto crece en promedio porcentualmente una variable cuando la otra crece 1%.

```{r}
ggplot(df, aes(x=num_beneficiarios_mean_pc, y=log10(pib_percapita))) +
  geom_point(aes(color=nome_regiao), size=.7) +
  geom_smooth(se=F, method="loess", size=.8) +
  facet_wrap(hierarquia ~ ., scales="free_y") +
  theme_minimal() +
  NULL
```

```{r}
ggplot(df, aes(x=num_beneficiarios_mean_pc, y=pib_percapita)) +
  geom_point(aes(color=nome_regiao), size=.8) +
  geom_smooth(se=F, method="lm") +
  facet_wrap(semiarido ~ nome_regiao, scales="free") +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  theme_minimal() +
  NULL
```


```{r}
df_tmp = df %>% 
  mutate(
    pibpc_log = log(pib_percapita), valorpc_log=log(suma_valor_pc)
  ) %>% 
  select(pibpc_log, valorpc_log, semiarido)
ggpubr::ggscatterhist(
  df_tmp, x="valorpc_log", y="pibpc_log", color="semiarido",
  size=1, alpha=0.6,  ggtheme=theme_minimal(),
  margin.params = list(fill="semiarido", color="black", size=0.2)
  # ,margin.plot="boxplot"
) 
```


### Barras

```{r}
ggplot(df) +
  geom_bar(aes(x=nome_regiao, fill=semiarido)) +
  NULL
```


```{r}
ggplot(df) +
  geom_bar(aes(x=nome_regiao, fill=semiarido), position="fill") +
  NULL
```


```{r}
ggplot(df) +
  geom_bar(aes(x=nome_regiao, fill=semiarido), position="dodge") +
  NULL
```

Con medidas resumen como la media:

```{r}
ggplot(df) +
  geom_bar(aes(x=nome_regiao, y=pib_percapita), 
           fun="mean", stat="summary", alpha=.5) + 
  theme_minimal() + 
  NULL
```

Los puntos suelen usarse para representar valores donde no es necesario incluir el 0.

```{r}
ggplot(df, aes(x=nome_regiao, y=pib_percapita)) +
  geom_point(fun="mean", stat="summary") +
  # stat_summary(fun = "mean", geom = "point") +
  geom_line(aes(group=1), fun="mean", stat="summary") +
  theme_minimal() + 
  NULL
```

Ordenando los niveles según los valores de otra variable:

```{r}
# resumir los datos y luego usar geom_col() puede ser mas sencillo:
df_tmp = df %>% 
  group_by(nome_uf) %>% 
  summarise_at(vars(populacao), sum)
ggplot(df_tmp) +
  geom_col(
    aes(x=populacao, y=fct_reorder(as.factor(nome_uf), populacao)), alpha=0.5) +
  labs(y=NULL) +
  scale_x_continuous(labels=scales::comma) +
  theme_classic() +
  NULL
```



```{r}
# La otra manera:
ggplot(df) +
  geom_bar(
    aes(x=populacao, y=fct_reorder(as.factor(nome_uf), populacao, .fun=sum)), 
    fun="sum", stat="summary", alpha=0.5) +
  labs(y=NULL) +
  scale_x_continuous(labels = scales::comma) +
  theme_classic() +
  NULL
```

#### Para pensar

Vamos a suponer momentáneamente que nuestro target es binario: PIB bajo (0) o PIB alto (1). ¿Qué tablas y visualizaciones nos pueden servir como EDA en este escenario?

```{r}
df = df %>% 
  mutate(target_pib = ifelse(pib_percapita > mean(pib_percapita), 1, 0))
```

Y: target_pib
X: ...

```{r}
table(df$target_pib)
```

```{r}
df_tmp = df %>% 
  group_by(target_pib) %>% 
  summarise(n = n())
ggplot(df_tmp) +
  geom_col(aes(x=as.factor(target_pib), y=n), alpha=0.5) +
  NULL
```


```{r}
ggplot(df, aes(x=num_beneficiarios_mean_pc, y=target_pib)) + 
  geom_point(size=0.5, alpha=0.5) +
  geom_smooth(method="lm", se=F) +
  ylim(c(-0.05, NA)) +
  NULL
```


```{r}
ggplot(df, aes(x=num_beneficiarios_mean_pc, fill=factor(target_pib))) +
  geom_density(alpha=0.5, adjust=2) +
  NULL
```


```{r}
ggplot(df, aes(x=factor(target_pib), y=num_beneficiarios_mean_pc)) +
  geom_boxplot() + 
  # scale_y_continuous(trans='log10') +
  # facet_wrap(~ tipologia) +
  # theme_minimal() +
  NULL
```

```{r}
df$target_pib %>% mean()
```


```{r}
df %>%
  group_by(semiarido) %>% 
  summarise(
    promedio = mean(target_pib),
    n = n()
  )
```

```{r}
table(df$target_pib)
mean(df$target_pib) * 100
```

```{r}
df_tmp = df %>%
  group_by(semiarido) %>% 
  summarise(
    porcentaje = mean(target_pib) * 100,
    n = n()
  )

ggplot(df_tmp) +
  geom_col(aes(x=semiarido, y=porcentaje), alpha=0.5) +
  geom_hline(yintercept=mean(df$target_pib) * 100, color="red",
             linetype="dashed") +
  NULL
```


```{r}

ggplot(df, aes(x=num_beneficiarios_mean_pc, y=log10(densidad), 
               color=as.factor(target_pib))) + 
  geom_point(size=1.0, alpha=0.5) +
  NULL


```













...

### Etc.

Con `GGally::ggpairs()` podemos analizar relaciones entre variables de múltiples tipos.

```{r, warning=F}
df_reduced = df %>%
  select(
    nome_regiao, suma_perbenef, densidad,
    despesa_ciencia_tecnologia_pc, pib_percapita, semiarido
  ) %>% 
  mutate_if(is.numeric, function(x) log10(x+1))
GGally::ggpairs(
  df_reduced, 
  columns = c("suma_perbenef", "densidad",
            "despesa_ciencia_tecnologia_pc", "pib_percapita"),
  mapping = aes(color=semiarido),
  lower = list(continuous = GGally::wrap("points", alpha=0.3, size=0.1))
)
```

Con `plotly::ggplotly()` podemos hacer interactivo cualquier gráfico de ggplot2.

```{r}
g = ggplot(
  df
  ,aes(x=log(pib_percapita+1), y=log(exportacoes+1)
       ,color=semiarido, label=municipio, label2=nome_regiao)) +
  geom_point(alpha=0.8, size=0.8) +
  theme_minimal() + 
  NULL
plotly::ggplotly(g)
```

## Mapas

Descargamos los shapefiles de [IBGE](https://geoftp.ibge.gov.br/organizacao_do_territorio/malhas_territoriais/malhas_municipais/municipio_2019/Brasil/BR/). 

Shapefile es un formato de archivo con información geográfica. Indican el sistemas de coordenadas de referencia y proyección cartográfica usados. Contiene puntos (posiciones), líneas (recorridos) y polígonos (superficies) de alguna parte del mundo.

Los podemos leer con `sf::read_sf()` y hacer un _join_ con nuestros datos.

```{r}
sf_mun = sf::read_sf(
  "data/raw/ibge/br_municipios_20200807/BR_Municipios_2019.shp")
sf_uf = sf::read_sf(
  "data/raw/ibge/br_unidades_da_federacao/BR_UF_2019.shp")
```

```{r}
names(sf_mun)
dim(sf_mun)
```


```{r}
sf_mun = sf_mun %>%
  left_join(df, by=c("CD_MUN"="codigo_municipio"))
```

```{r}
# municipios faltantes en nuestros datos
sf_mun %>% 
  filter(is.na(municipio)) %>% 
  select(CD_MUN, NM_MUN, SIGLA_UF, AREA_KM2)
```

Creamos mapas coropléticos con `geom_sf()`. En estos mapas las áreas se sombrean de distintos colores según alguna característica.

```{r}
sf_mun_tmp = sf_mun %>% filter(nome_uf == "Sergipe")
sf_uf_tmp = sf_uf %>% filter(NM_UF == "Sergipe")

ggplot() +
  geom_sf(data=sf_mun_tmp, aes(fill=pib_percapita), color="black", size=0.5) +
  geom_sf(data=sf_uf_tmp, fill=NA, color="black", size=0.8) +
  scale_fill_viridis_c(trans="log10", na.value="gray60") +
  # coord_sf(xlim=c(-75,-55), ylim=c(-10, 3)) +
  labs(fill="PIB per capita") +
  NULL
```

```{r}
g = ggplot() +
  geom_sf(data=sf_mun, aes(fill=pib_percapita), color=NA) +
  geom_sf(data=sf_uf, fill=NA, color="black", size=0.5) +
  scale_fill_viridis_c(trans="log10", na.value="gray60") +
  theme_minimal() +
  guides(scale ="none") +
  theme(legend.position=c(0.15,0.2)) +
  labs(fill="PIB per capita") +
  NULL

ggsave("output/mapa_pibpercapita.png", g, width=10, height=8)
```

Si tuviéramos lat y long en nuestros datos podríamos inferir el mapa de esta manera:

```
df_geo = df %>% 
  st_as_sf(coords=c("longitud", "latitud"), crs=4326)

ggplot() + geom_sf(df_geo)
```

# Bonus track

¿Cómo generamos todos los gráficos bivariados con respecto a la variable target?

```{r}
# generamos log de todas las vars
df_num_expanded = df_num %>%
  mutate_all(list("log" = function(x) ifelse(x==0, 0, sign(x)*log10(abs(x))) ))
```

```{r}
# Medimos corr en logs vs el target (equivale a spearman!)
corr_mat = cor(
  df_num_expanded %>% select(ends_with("_log")) %>% select(-pib_percapita_log),
  df_num_expanded %>% select(pib_percapita_log),
  method="pearson", use="pairwise")
df_corr = corr_mat %>% as.data.frame()
names(df_corr) = "correlation"
df_corr = df_corr %>% 
  arrange(-abs(correlation)) %>% 
  rownames_to_column("variable")
```


```{r}
# plot function
plt_corr = function(df, variable) {
  p = ggplot(df, aes(x=!!sym(variable), y=pib_percapita_log)) +
    geom_jitter(size=.3, alpha=.2) +
    geom_smooth(se=T, method="loess", size=.8, formula=y~x) +
    labs(title=variable) +
    theme_minimal() +
    NULL
  return(p)
}
```


```{r}
# lista de plots
variables = df_corr$variable
list_plots = map(variables, function(x) plt_corr(df_num_expanded, x))
```



```{r, warning=F}
# save plots
ggsave("output/bivariado_pib.pdf",width=6, height=6,
       gridExtra::marrangeGrob(grobs=list_plots, nrow=2, ncol=1))
```

# Referencias

* [R for Data Science](https://r4ds.had.co.nz/)
* [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/)
* [RMarkdown for Scientists](https://rmd4sci.njtierney.com/)
* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
* [Hands-on Exploratory Data Analysis With Python](https://github.com/PacktPublishing/Hands-on-Exploratory-Data-Analysis-with-Python)
